# QPLEX Configuration for MATE 4v4 environment
name: "QPLEX_4v4_9"
env_name: "MATE-4v4-9"
seed: 42

# Environment settings
env:
  config_file: "mate/assets/MATE-4v4-9.yaml"
  max_episode_steps: 2000
  reward_type: "dense"
  render_mode: "human"  # "human" or "rgb_array"
  window_size: 800

# Training settings
training:
  total_timesteps: 120000
  learning_starts: 5000
  train_freq: 4
  target_update_interval: 200
  gradient_steps: 1
  batch_size: 32
  buffer_size: 40000
  n_episodes: 1000
  
# Algorithm settings
algorithm:
  name: "QPLEX"
  learning_rate: 0.0005
  gamma: 0.99
  tau: 0.005
  epsilon_start: 1.0
  epsilon_end: 0.05
  epsilon_decay: 0.9999
  dueling: true
  double_q: true
  
# Network architecture
network:
  # Individual Q-networks
  q_network:
    hidden_dims: [256, 256]
    use_rnn: false
    rnn_hidden_dim: 128
    rnn_layers: 2
    rnn_type: "lstm"  # "lstm" or "gru"
    use_attention: false
    num_attention_heads: 4
    dropout: 0.0
    activation: "relu"  # "relu", "tanh", "leaky_relu"
  
  # Mixing network
  mixing_network:
    hidden_dims: [256, 256]
    use_hypernet: true
    dropout: 0.0
    activation: "relu"
  
  # Target network update
  target_update:
    method: "soft"  # "soft" or "hard"
    tau: 0.005

# Agent settings
agents:
  n_agents: 4
  obs_dim: null  # Will be set automatically from environment
  action_dim: null  # Will be set automatically from environment
  state_dim: null  # Will be set automatically from environment
  
# Logging and evaluation
logging:
  log_interval: 2000
  eval_interval: 2000
  save_interval: 10000
  log_dir: "./logs/new_qplex"
  model_dir: "./models/new_qplex"
  tensorboard: true
  wandb: false
  wandb_project: "qplex-mate"
  wandb_entity: null

# Evaluation settings
evaluation:
  # Legacy settings (for backward compatibility with old evaluation method)
  n_eval_episodes: 10  # Number of episodes for simple evaluation
  eval_deterministic: true  # Use deterministic policy during evaluation
  render_eval: false  # Whether to render during evaluation
  save_videos: false  # Whether to save evaluation videos
  video_dir: "./videos"  # Directory to save videos
  
  # ============================================================================
  # ImprovedEvaluator Settings (for 2-group evaluation with statistical analysis)
  # ============================================================================
  # These settings enable robust evaluation with multiple runs, outlier removal,
  # and confidence intervals to reduce variance and improve result stability.
  
  # Basic evaluation parameters
  n_eval_runs: 5  # Number of independent evaluation runs with different seeds
                  # More runs = more stable results but longer evaluation time
                  # Recommended: 3-10 runs depending on variance tolerance
  
  n_episodes_per_run: 400  # Number of episodes per evaluation run
                           # Total episodes = n_eval_runs × n_episodes_per_run
                           # Example: 5 runs × 400 episodes = 2000 total episodes
                           # Recommended: 200-500 episodes per run
  
  n_warmup_episodes: 10  # Number of warm-up episodes before collecting metrics
                         # Helps stabilize RNN hidden states and agent behavior
                         # These episodes are not counted in final statistics
                         # Recommended: 5-20 episodes, more if using RNN
  
  batch_size: 50  # Batch size for evaluation (reserved for future optimization)
                  # Currently not used but may enable batched evaluation later
                  # Recommended: 25-100 depending on memory constraints
  
  # Statistical analysis settings
  remove_outliers: true  # Whether to remove statistical outliers from results
                         # Helps reduce impact of anomalous episodes on metrics
                         # Recommended: true for more stable and reliable results
  
  outlier_method: 'iqr'  # Method for detecting outliers:
                         # - 'iqr': Interquartile Range method (robust, recommended)
                         #   Removes values outside [Q1 - threshold×IQR, Q3 + threshold×IQR]
                         # - 'zscore': Z-score method (assumes normal distribution)
                         #   Removes values with |z-score| > threshold
  
  outlier_threshold: 1.5  # Threshold for outlier detection
                          # For IQR method: 1.5 is standard (moderate), 3.0 is conservative
                          # For zscore method: 3.0 is standard (99.7% of data retained)
                          # Lower values = more aggressive outlier removal
  
  confidence_level: 0.95  # Confidence level for confidence intervals (0-1)
                          # 0.95 = 95% confidence interval (standard in research)
                          # 0.99 = 99% confidence interval (more conservative)
                          # Results will include mean ± confidence interval
  
  # Reproducibility settings
  seeds: [42, 142, 242, 342, 442]  # Random seeds for each evaluation run
                                    # One seed per run (length must equal n_eval_runs)
                                    # If null, seeds will be auto-generated as [42, 142, 242, ...]
                                    # Using fixed seeds ensures reproducible results
  
  # Group evaluation settings
  n_groups: 2  # Number of agent groups to evaluate separately
               # Currently fixed at 2 for comparing two different agent configurations
               # Each group gets independent metrics and statistical analysis

# Device settings
device:
  use_cuda: true
  cuda_device: 0

# Reproducibility
reproducibility:
  deterministic: true
  benchmark: false
